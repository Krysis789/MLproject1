{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data into feature matrix, class labels, and event ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj1_helpers import *\n",
    "DATA_TRAIN_PATH = '../../data/train.csv' # TODO: download train data and supply path here \n",
    "y, tX, ids = load_csv_data(DATA_TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do your thing crazy machine learning thing here :) ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Implementations of the required function\n",
    "\n",
    "#Linear regression using gradient descent\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "    nIters = 0 #Keep count of iterations\n",
    "    w = initial_w #Current weights\n",
    "    n = len(y) #Number of observations\n",
    "    while (nIters < max_iters):\n",
    "        e = y - np.dot(tx, w) #Residual vector\n",
    "        gradient = -np.dot(np.transpose(tx), e) / n #Gradient\n",
    "        w -= gamma * gradient #A step towards negative gradient\n",
    "        nIters += 1 #Update number of iterations\n",
    "    e = y - np.dot(tx, w) #Compute final residuals\n",
    "    return w, np.dot(np.transpose(e), e) / (2 * n) #Return weights and loss (2n as a scaler)\n",
    "\n",
    "#Linear regression using stochastic gradient descent\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    nIters = 0 #Keep track of iterations\n",
    "    w = initial_w #Update w\n",
    "    n = len(y) #Number of datapoints\n",
    "    while (nIters < max_iters):\n",
    "        index = np.random.randint(0, n) #Pick a row uniformly\n",
    "        row = tx[index, :] #Select the chosen row \n",
    "        e = y[index] - np.dot(row, w) #Calculate the estimate for error\n",
    "        gradient = -np.dot(np.transpose(row), e) #Calculate the estimate for the gradient\n",
    "        w -= gamma * gradient #Update w\n",
    "        nIters += 1 #Update number of iterations\n",
    "    e = y - np.dot(tx, w) #Calculate the residuals for the final loss\n",
    "    return w, np.dot(np.transpose(e), e) / (2 * n) #Return the weights and the loss (2n as a scaler)\n",
    "\n",
    "#Least squares regression using normal equations\n",
    "def least_squares(y, tx):\n",
    "    xtx = np.dot(np.transpose(tx), tx) #Calculate the Gram matrix\n",
    "    w = np.dot(np.dot(np.linalg.inv(xtx), np.transpose(tx)), y) #Calculate the weigths\n",
    "    e = y - np.dot(tx, w) #Calculate the residuals\n",
    "    loss = np.dot(np.transpose(e), e) / (2 * len(y)) #Calculate the loss (2n as a scaler)\n",
    "    return w, loss\n",
    "\n",
    "#Ridge regression using normal equations\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    xtx = np.dot(np.transpose(tx), tx) + lambda_ * np.identity(np.shape(tx)[1]) #Calculate the modified Gram matrix\n",
    "    w = np.dot(np.dot(np.linalg.inv(xtx), np.transpose(tx)), y) #Calculate the weigths\n",
    "    e = y - np.dot(tx, w) #Calculate residuals\n",
    "    loss = np.dot(np.transpose(e), e) / (2 * len(y)) #Calculate the loss (2n as a scaler)\n",
    "    return w, loss\n",
    "\n",
    "#A helper for the logistic regression\n",
    "def sigmoid(x):\n",
    "    return np.exp(x) / (1 + np.exp(x))\n",
    "\n",
    "#Logistic regression using gradient descent\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    nIters = 0\n",
    "    w = initial_w\n",
    "    n = len(y)\n",
    "    while (nIters < max_iters): \n",
    "        gradient = np.dot(np.transpose(tx), sigmoid(np.dot(tx, w)) - y)\n",
    "        w -= gamma * gradient #Calculate new w\n",
    "        nIters += 1 #Update number of iterations\n",
    "    loss = np.sum(np.log(1 + np.exp(np.dot(tx, w))) - y * np.dot(tx, w))\n",
    "    return w, loss #Return weights and loss\n",
    "\n",
    "#Regularized logistic regression using gradient descent or SGD\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \n",
    "    nIters = 0\n",
    "    w = initial_w\n",
    "    n = len(y)\n",
    "    while (nIters < max_iters): \n",
    "        \n",
    "        #calculate the hessian matrix\n",
    "        S = np.zeros(n)\n",
    "        for k in range(n):\n",
    "            sig = sigmoid(np.dot(np.transpose(tx[k]), w))\n",
    "            S[k] = sig * (1 - sig)\n",
    "        \n",
    "        tmp = np.zeros((n, np.shape(tx)[1]))\n",
    "        for k in range(n):\n",
    "            tmp[k, :] = tx[k, :] * S[k]\n",
    "        hessian = np.dot(np.transpose(tx), tmp)\n",
    "        \n",
    "        #calculate the gradient\n",
    "        gradient = np.dot(np.transpose(tx), sigmoid(np.dot(tx, w)) - y)\n",
    "        \n",
    "        #update w\n",
    "        w = w - gamma*np.dot(np.linalg.inv(hessian),gradient)\n",
    "        \n",
    "        nIters += 1 #Update number of iterations\n",
    "        \n",
    "    #calculate the loss\n",
    "    loss = 0\n",
    "    for k in range(n):\n",
    "        loss += np.log(1 + np.exp(np.dot(np.transpose(tx[k]),w))) - np.dot(y[k],np.dot(np.transpose(tx[k]),w))\n",
    "    loss += lambda_/2* np.linalg.norm(w)\n",
    "    \n",
    "    return w, loss #Return weights and loss\n",
    "\n",
    "## Tests:\n",
    "#w1, loss1 = least_squares(y, tX)\n",
    "#w2, loss2 = ridge_regression(y, tX, 0.02)\n",
    "#w3, loss3 = least_squares_GD(y, tX, 10 * w2, 20, 0.01)\n",
    "#w4, loss4 = least_squares_SGD(y, tX, np.ones(30), 10, 0.01)\n",
    "#y2 = np.copy(y)\n",
    "#y2[y2 == -1] = 0\n",
    "#w4, loss4 = logistic_regression(y2, tX, np.ones(30), 10, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for cleaning and manipulating the data\n",
    "def replace_with_mean(data, replacedValue):\n",
    "    means = []\n",
    "    def replace_one_col(x):\n",
    "        m = np.mean(x[x != replacedValue])\n",
    "        x[x == replacedValue] = m\n",
    "        means.append(m)\n",
    "        return x\n",
    "    return np.apply_along_axis(replace_one_col, 0, np.copy(data)), means\n",
    "\n",
    "def replace_with_values(data, values, replacedValue):\n",
    "    result = np.copy(data)\n",
    "    for i in range(np.shape(data)[1]):\n",
    "        for j in range(np.shape(data)[0]):\n",
    "            if result[j, i] == replacedValue:\n",
    "                result[j, i] = values[i]\n",
    "    return result\n",
    "            \n",
    "def centralize_data(data):\n",
    "    return np.apply_along_axis(lambda x: x - np.mean(x), 0, np.copy(data))\n",
    "\n",
    "def subtract_values(data, values):\n",
    "    result = np.copy(data)\n",
    "    for i in range(np.shape(data)[1]):\n",
    "        for j in range(np.shape(data)[0]):\n",
    "            result[j, i] = result[j, i] - values[i]\n",
    "    return result\n",
    "\n",
    "\n",
    "def scale_data(data):\n",
    "    deviations = np.apply_along_axis(lambda x: np.std(x), 0, np.copy(data))\n",
    "    return np.apply_along_axis(lambda x: x / np.std(x), 0, np.copy(data)), deviations\n",
    "\n",
    "def divide_by_values(data, values):\n",
    "    result = np.copy(data)\n",
    "    for i in range(np.shape(data)[1]):\n",
    "        for j in range(np.shape(data)[0]):\n",
    "            result[j, i] = result[j, i] / values[i]\n",
    "    return result\n",
    "\n",
    "def add_constant_term(data):\n",
    "    a = np.ones((np.shape(data)[0], np.shape(data)[1] + 1))\n",
    "    a[:, :-1] = data\n",
    "    return a\n",
    "\n",
    "def add_missing_value_info(data, missingValue):\n",
    "    a = np.zeros((np.shape(data)[0], np.shape(data)[1] + 1))\n",
    "    a[:, :-1] = data\n",
    "    a[:, np.shape(a)[1] - 1] = np.apply_along_axis(lambda x: np.sum(x == missingValue), 1, data)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for model validation\n",
    "def split_data(x, y, ratio, seed = 1):\n",
    "    #Split the dataset based on the split ratio.\n",
    "    np.random.seed(seed) #Set a seed for reproducilbility\n",
    "    trainSize = int(np.round(len(y) * ratio)) #Calculate the desired size of the train data set \n",
    "    trainIndexes = np.random.choice(len(y), trainSize, False) #Sample the training data\n",
    "    testIndexes = np.setdiff1d(np.arange(len(y)), trainIndexes, assume_unique=True) #Select the test data\n",
    "    return x[trainIndexes, :], y[trainIndexes], x[testIndexes, :], y[testIndexes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test data\n",
    "DATA_TEST_PATH = '../../data/test.csv'\n",
    "_, tX_test, ids_test = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX1, means = replace_with_mean(tX, -999)\n",
    "tX1, deviations = scale_data(centralize_data(tX1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss ratio1:  0.9999968428610891\n",
      "Loss ratio2:  0.989583637460409\n",
      "[ 9.63655762e-03 -2.54736019e-01 -2.63503443e-01 -1.16337530e-03\n",
      "  2.18326773e-02  9.00606246e-02  4.84226674e-03  2.82008481e-01\n",
      " -2.81645647e-02  5.56983585e-02 -1.88136508e-01  1.18049028e-01\n",
      "  7.66256122e-02  1.74657749e-01 -7.79826766e-04 -8.30897863e-04\n",
      "  2.78600745e-01 -8.61053861e-04  2.51520388e-03  1.03680159e-01\n",
      "  9.36298625e-04 -4.70340922e-02  4.08260971e-02 -4.81893744e-02\n",
      "  6.52475974e-04  1.89116701e-04 -3.69103668e-02  1.55888050e-03\n",
      " -1.74013359e-03 -3.69016108e-02]\n",
      "[ 0.0181864  -0.25927543 -0.25542027 -0.00553715  0.04847504  0.07236159\n",
      " -0.02086306  0.26175669 -0.01584657  0.04067617 -0.15919671  0.08658388\n",
      "  0.08730788  0.17664583  0.00746078 -0.00470276  0.2789183  -0.00586078\n",
      " -0.02104699  0.14647441 -0.01099862 -0.01614169  0.07305421 -0.05265507\n",
      " -0.0238358   0.00175934 -0.0575094  -0.01145926  0.00138146 -0.05516273]\n",
      "[ 9.63468389e-03 -2.54718967e-01 -2.63502819e-01 -1.10015735e-03\n",
      "  2.18421784e-02  9.00540130e-02  4.83490122e-03  2.82008718e-01\n",
      " -2.81501517e-02 -3.29252236e+02 -1.88141126e-01  1.18065059e-01\n",
      "  7.66172496e-02  6.39610454e+01 -7.79460235e-04 -8.30655642e-04\n",
      "  6.30769551e+01 -8.61168635e-04  2.51791327e-03  1.03659312e-01\n",
      "  9.33786268e-04 -4.70019019e-02  4.17575954e-02 -4.75783464e-02\n",
      "  6.50726491e-04  1.88755856e-04 -3.66001847e-02  1.55837341e-03\n",
      " -1.74318654e-03  2.78921323e+02]\n",
      "Max weight difference1:  278.9764853077811\n",
      "Max weight difference1:  278.95822419188437\n"
     ]
    }
   ],
   "source": [
    "### Fit models\n",
    "\n",
    "#We compare different models using the prediction accuracies. The comparison is done using cross validation.\n",
    "#We first compare whetrher the GD, SGD and the closed least square functions give the same models. Then we fit a ridge regression model\n",
    "#using different regularization parameters and finally a logistic regression model and regularized logistic regression\n",
    "#models using different parameters. The best model is chosen using cross validation.\n",
    "\n",
    "\n",
    "##Linear models:\n",
    "#The models are fitted to the preprocessed and centralized data and a learning rate of 0.1\n",
    "#is used in the GD and SGD algorithms.\n",
    "\n",
    "#1. A model using GD\n",
    "weightsGD, lossGD = least_squares_GD(y, tX1, np.zeros(np.shape(tX1)[1]), 2000, 0.1)\n",
    "\n",
    "#2. A model using SGD\n",
    "weightsSGD, lossSGD = least_squares_SGD(y, tX1, np.zeros(np.shape(tX1)[1]), 200000, 0.001)\n",
    "\n",
    "#3. A model using closed form solution for the least squares\n",
    "weightsLSQ, lossLSQ = least_squares(y, tX1)\n",
    "\n",
    "#Check for differences in losses and weights\n",
    "\n",
    "print(\"Loss ratio1: \", lossLSQ / lossGD)\n",
    "print(\"Loss ratio2: \", lossLSQ / lossSGD)\n",
    "print(weightsGD)\n",
    "print(weightsSGD)\n",
    "print(weightsLSQ)\n",
    "print(\"Max weight difference1: \", np.max(weightsLSQ - weightsSGD))\n",
    "print(\"Max weight difference1: \", np.max(weightsLSQ - weightsGD))\n",
    "\n",
    "#The results are close to each other. In the future, only least_squares will be used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In this part, we will fit a model using ridge regression and try to find an optimal \n",
    "#regularization parameter using 5 fold cross validation. The used split ratio is 0.9.\n",
    "\n",
    "#The losses will be calculated using prediction accuracy with a threshold of 0,\n",
    "#as the final model will be rated using this same criterion.\n",
    "\n",
    "folds = 5\n",
    "lambdas = np.logspace(-4, 0, 30)\n",
    "accuracies = np.zeros((len(lambdas), folds))\n",
    "\n",
    "np.random.seed(123)\n",
    "for k in range(folds):\n",
    "    i = 0\n",
    "    trainX, trainY, testX, testY = split_data(tX, y, 0.9, seed = 1)\n",
    "    for lambda_ in lambdas:\n",
    "        weights, _ = ridge_regression(trainY, trainX, lambda_)\n",
    "        predictions = np.dot(testX, weights)\n",
    "        predictions[predictions < 0] = -1\n",
    "        predictions[predictions >= 0] = 1\n",
    "        #Use a naive loss function (prediction accuracy)\n",
    "        accuracy = np.mean(predictions == testY)\n",
    "        accuracies[i, k] = accuracy\n",
    "        i += 1\n",
    "\n",
    "# Select the model with the highest mean accuracy to be used later (some of the smallest values seem to be as good)\n",
    "# It seems that the model performs best when the lambda is close to zero.\n",
    "ridgeLambda = lambdas[np.argmax(np.mean(accuracies, axis = 1))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "New lambda\n",
      "New lambda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-c93bcc37040c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlambda_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"New lambda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mpredictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredictions\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-079e4be83f2e>\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0msig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msig\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##Next, regularized logistic regression models will be fitted using the same kind of process as with the ridge regression.\n",
    "##However, as the optimization algorithm is quite slow, only a 2 fold cross validation is performed and the search space is\n",
    "#smaller\n",
    "\n",
    "folds = 2\n",
    "initialW = np.zeros(np.shape(trainX)[1])\n",
    "lambdas = np.logspace(-4, 0, 10)\n",
    "accuracies = np.zeros((len(lambdas), folds))\n",
    "\n",
    "np.random.seed(123)\n",
    "for k in range(folds):\n",
    "    i = 0\n",
    "    trainX, trainY, testX, testY = split_data(tX, y, 0.9, seed = 1)\n",
    "    trainX[trainX == -1] = 0\n",
    "    trainY[trainY == -1] = 0\n",
    "    print(k)\n",
    "    \n",
    "    for lambda_ in lambdas:\n",
    "        print(\"New lambda\")\n",
    "        weights, _ = reg_logistic_regression(trainY, trainX, lambda_, initialW, 20, 0.1)\n",
    "        predictions = np.dot(testX, weights)\n",
    "        predictions[predictions < 0.5] = 0\n",
    "        predictions[predictions > 0] = 1\n",
    "        #Use a naive loss function (prediction accuracy)\n",
    "        accuracy = np.mean(predictions == testY)\n",
    "        accuracies[i, k] = accuracy\n",
    "        i += 1\n",
    "\n",
    "# Select the model with the highest mean accuracy to be used later\n",
    "regLogRegLambda = lambdas[np.argmax(np.mean(accuracies, axis = 1))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Lsq\n",
      "Ridge\n",
      "Logistic regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alvari.Kone\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Alvari.Kone\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in greater\n",
      "C:\\Users\\Alvari.Kone\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:42: RuntimeWarning: invalid value encountered in less_equal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularized logistic regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alvari.Kone\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:49: RuntimeWarning: invalid value encountered in greater\n",
      "C:\\Users\\Alvari.Kone\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:50: RuntimeWarning: invalid value encountered in less_equal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lsq\n",
      "Ridge\n",
      "Logistic regression\n",
      "Regularized logistic regression\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-8b505f77cd4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Regularized logistic regression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m#Regularized logistic regression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mregLogRegWeights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreg_logistic_regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregLogRegLambda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitialW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mlogisticPreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregLogRegWeights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mlogisticPreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlogisticPreds\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-079e4be83f2e>\u001b[0m in \u001b[0;36mreg_logistic_regression\u001b[1;34m(y, tx, lambda_, initial_w, max_iters, gamma)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m             \u001b[0mtmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mhessian\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##At this section, we are comparing different regression models using cross validation\n",
    "##The compared models are:\n",
    "#1: Linear regression model fitted by the least_squares function\n",
    "#2: Ridge regression using the lambda found earlier\n",
    "#3: Regularized logistic regression using the lambda found earlier\n",
    "#4: A logistic regression model\n",
    "\n",
    "folds = 5\n",
    "initialW = np.zeros(np.shape(trainX)[1])\n",
    "lambdas = np.logspace(-4, 0, 30)\n",
    "accuracies = np.zeros((4, folds))\n",
    "\n",
    "np.random.seed(200)\n",
    "for k in range(folds):\n",
    "    print(k)\n",
    "    trainX, trainY, testX, testY = split_data(tX1, y, 0.9, seed = 1)\n",
    "    for lambda_ in lambdas:\n",
    "        \n",
    "        print(\"Lsq\")\n",
    "        #Least squares linear regression\n",
    "        lsqWeights, _ = least_squares(trainY, trainX)\n",
    "        lsqPreds = np.dot(testX, lsqWeights)\n",
    "        lsqPreds[lsqPreds < 0] = -1\n",
    "        lsqPreds[lsqPreds >= 0] = 1\n",
    "        accuracies[0, k] = np.mean(lsqPreds == testY)\n",
    "        \n",
    "        print(\"Ridge\")\n",
    "        #Penalized linear regression\n",
    "        ridgeWeights, _ = ridge_regression(trainY, trainX, ridgeLambda)\n",
    "        ridgePreds = np.dot(testX, ridgeWeights)\n",
    "        ridgePreds[ridgePreds < 0] = -1\n",
    "        ridgePreds[ridgePreds >= 0] = 1\n",
    "        accuracies[1, k] = np.mean(ridgePreds == testY)\n",
    "        \n",
    "        print(\"Logistic regression\")\n",
    "        #Logistic regression\n",
    "        trainY[trainY == -1] = 0\n",
    "        testY[testY == -1] = 0\n",
    "        logisticWeights, _ = logistic_regression(trainY, trainX, initialW, 20, 0.1)\n",
    "        logisticPreds = sigmoid(np.dot(testX, logisticWeights))\n",
    "        logisticPreds[logisticPreds > 0.5] = 1\n",
    "        logisticPreds[logisticPreds <= 0.5] = 0\n",
    "        accuracies[2, k] = np.mean(logisticPreds == testY)\n",
    "        \n",
    "        print(\"Regularized logistic regression\")\n",
    "        #Regularized logistic regression\n",
    "        regLogRegWeights, _ = reg_logistic_regression(trainY, trainX, regLogRegLambda, initialW, 20, 0.1)\n",
    "        logisticPreds = sigmoid(np.dot(testX, regLogRegWeights))\n",
    "        logisticPreds[logisticPreds > 0.5] = 1\n",
    "        logisticPreds[logisticPreds <= 0.5] = 0\n",
    "        accuracies[3, k] = np.mean(logisticPreds == testY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#According to the cross validation, the best model is ???\n",
    "#Fit this model using the whole training data and do predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the optimal weights (lambda = 0.046415888) to make predictions for the real test data\n",
    "tmp = replace_with_values(add_missing_value_info(tX_test, -999), means, -999)\n",
    "tmp[tmp[:, np.shape(tmp)[1] -1] > 0, np.shape(tmp)[1] -1] = 1\n",
    "model2TestData = add_constant_term(divide_by_values(subtract_values(tmp, means), deviations))\n",
    "testPredictions = sigmoid(np.dot(model2TestData, bestW))\n",
    "testPredictions[testPredictions < 0.5] = -1\n",
    "testPredictions[testPredictions > 0] = 1\n",
    "#Save predictions\n",
    "OUTPUT_PATH = '../../predictions/regularized_logistic_regression2.csv' \n",
    "create_csv_submission(ids_test, testPredictions, OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
